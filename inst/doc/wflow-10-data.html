<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />

<meta name="viewport" content="width=device-width, initial-scale=1" />

<meta name="author" content="John Blischak" />

<meta name="date" content="2023-08-22" />

<title>Using large data files with workflowr</title>

<script>// Pandoc 2.9 adds attributes on both header and div. We remove the former (to
// be compatible with the behavior of Pandoc < 2.8).
document.addEventListener('DOMContentLoaded', function(e) {
  var hs = document.querySelectorAll("div.section[class*='level'] > :first-child");
  var i, h, a;
  for (i = 0; i < hs.length; i++) {
    h = hs[i];
    if (!/^h[1-6]$/i.test(h.tagName)) continue;  // it should be a header h1-h6
    a = h.attributes;
    while (a.length > 0) h.removeAttribute(a[0].name);
  }
});
</script>
<script>// Hide empty <a> tag within highlighted CodeBlock for screen reader accessibility (see https://github.com/jgm/pandoc/issues/6352#issuecomment-626106786) -->
// v0.0.1
// Written by JooYoung Seo (jooyoung@psu.edu) and Atsushi Yasumoto on June 1st, 2020.

document.addEventListener('DOMContentLoaded', function() {
  const codeList = document.getElementsByClassName("sourceCode");
  for (var i = 0; i < codeList.length; i++) {
    var linkList = codeList[i].getElementsByTagName('a');
    for (var j = 0; j < linkList.length; j++) {
      if (linkList[j].innerHTML === "") {
        linkList[j].setAttribute('aria-hidden', 'true');
      }
    }
  }
});
</script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>







<style type="text/css">body {
background-color: #fff;
margin: 1em auto;
max-width: 700px;
overflow: visible;
padding-left: 2em;
padding-right: 2em;
font-family: "Open Sans", "Helvetica Neue", Helvetica, Arial, sans-serif;
font-size: 14px;
line-height: 1.35;
}
#TOC {
clear: both;
margin: 0 0 10px 10px;
padding: 4px;
width: 400px;
border: 1px solid #CCCCCC;
border-radius: 5px;
background-color: #f6f6f6;
font-size: 13px;
line-height: 1.3;
}
#TOC .toctitle {
font-weight: bold;
font-size: 15px;
margin-left: 5px;
}
#TOC ul {
padding-left: 40px;
margin-left: -1.5em;
margin-top: 5px;
margin-bottom: 5px;
}
#TOC ul ul {
margin-left: -2em;
}
#TOC li {
line-height: 16px;
}
table {
margin: 1em auto;
border-width: 1px;
border-color: #DDDDDD;
border-style: outset;
border-collapse: collapse;
}
table th {
border-width: 2px;
padding: 5px;
border-style: inset;
}
table td {
border-width: 1px;
border-style: inset;
line-height: 18px;
padding: 5px 5px;
}
table, table th, table td {
border-left-style: none;
border-right-style: none;
}
table thead, table tr.even {
background-color: #f7f7f7;
}
p {
margin: 0.5em 0;
}
blockquote {
background-color: #f6f6f6;
padding: 0.25em 0.75em;
}
hr {
border-style: solid;
border: none;
border-top: 1px solid #777;
margin: 28px 0;
}
dl {
margin-left: 0;
}
dl dd {
margin-bottom: 13px;
margin-left: 13px;
}
dl dt {
font-weight: bold;
}
ul {
margin-top: 0;
}
ul li {
list-style: circle outside;
}
ul ul {
margin-bottom: 0;
}
pre, code {
background-color: #f7f7f7;
border-radius: 3px;
color: #333;
white-space: pre-wrap; 
}
pre {
border-radius: 3px;
margin: 5px 0px 10px 0px;
padding: 10px;
}
pre:not([class]) {
background-color: #f7f7f7;
}
code {
font-family: Consolas, Monaco, 'Courier New', monospace;
font-size: 85%;
}
p > code, li > code {
padding: 2px 0px;
}
div.figure {
text-align: center;
}
img {
background-color: #FFFFFF;
padding: 2px;
border: 1px solid #DDDDDD;
border-radius: 3px;
border: 1px solid #CCCCCC;
margin: 0 5px;
}
h1 {
margin-top: 0;
font-size: 35px;
line-height: 40px;
}
h2 {
border-bottom: 4px solid #f7f7f7;
padding-top: 10px;
padding-bottom: 2px;
font-size: 145%;
}
h3 {
border-bottom: 2px solid #f7f7f7;
padding-top: 10px;
font-size: 120%;
}
h4 {
border-bottom: 1px solid #f7f7f7;
margin-left: 8px;
font-size: 105%;
}
h5, h6 {
border-bottom: 1px solid #ccc;
font-size: 105%;
}
a {
color: #0033dd;
text-decoration: none;
}
a:hover {
color: #6666ff; }
a:visited {
color: #800080; }
a:visited:hover {
color: #BB00BB; }
a[href^="http:"] {
text-decoration: underline; }
a[href^="https:"] {
text-decoration: underline; }

code > span.kw { color: #555; font-weight: bold; } 
code > span.dt { color: #902000; } 
code > span.dv { color: #40a070; } 
code > span.bn { color: #d14; } 
code > span.fl { color: #d14; } 
code > span.ch { color: #d14; } 
code > span.st { color: #d14; } 
code > span.co { color: #888888; font-style: italic; } 
code > span.ot { color: #007020; } 
code > span.al { color: #ff0000; font-weight: bold; } 
code > span.fu { color: #900; font-weight: bold; } 
code > span.er { color: #a61717; background-color: #e3d2d2; } 
</style>




</head>

<body>




<h1 class="title toc-ignore">Using large data files with workflowr</h1>
<h3 class="subtitle">workflowr version 1.7.1</h3>
<h4 class="author">John Blischak</h4>
<h4 class="date">2023-08-22</h4>


<div id="TOC">
<ul>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#option-0-reconsider-versioning-your-large-data-files">Option 0: Reconsider versioning your large data files</a></li>
<li><a href="#option-1-record-metadata">Option 1: Record metadata</a></li>
<li><a href="#option-2-use-git-lfs-large-file-storage">Option 2: Use Git LFS (Large File Storage)</a></li>
<li><a href="#option-3-use-piggyback">Option 3: Use piggyback</a></li>
<li><a href="#option-4-use-a-database">Option 4: Use a database</a></li>
</ul>
</div>

<div id="introduction" class="section level2">
<h2>Introduction</h2>
<p>Workflowr provides many features to track the progress of your data analysis project and make it easier to reproduce both the current version as well as previous versions of the project. However, this is only possible if the data files from previous versions can also be restored. In other words, even if you can obtain the code from six months ago, if you can’t obtain the data from six months ago, you won’t be able to reproduce your previous analysis.</p>
<p>Unfortunately, if you have large data files, you can’t simply commit them to the Git repository along with the code. The max file size able to be pushed to GitHub is <a href="https://help.github.com/en/github/managing-large-files/conditions-for-large-files">100 MB</a>, and this is in general a good practice to follow no matter what Git hosting service you are using. Large files will make each push and pull take much longer and increase the risk of the download timing out. This vignette discusses various strategies for versioning your large data files.</p>
</div>
<div id="option-0-reconsider-versioning-your-large-data-files" class="section level2">
<h2>Option 0: Reconsider versioning your large data files</h2>
<p>Before considering any of the options below, you need to reconsider if this is even necessary for your project. And if it is, which data files need to be versioned. Specifically, large raw data files that are never modified do not need to be versioned. Instead, you could follow these steps:</p>
<ol style="list-style-type: decimal">
<li>Upload the files to an online data repository, a private FTP server, etc.</li>
<li>Add a script to your workflowr project that can download all the files</li>
<li>Include the instructions in your README and your workflowr website that explain how to download the files</li>
</ol>
<p>For example, an <a href="https://en.wikipedia.org/wiki/RNA-Seq">RNA sequencing</a> project will produce <a href="https://en.wikipedia.org/wiki/FASTQ_format">FASTQ</a> files that are large and won’t be modified. Instead of committing these files to the Git repository, they should instead be uploaded to <a href="https://www.ncbi.nlm.nih.gov/geo/">GEO</a>/<a href="https://www.ncbi.nlm.nih.gov/sra">SRA</a>.</p>
</div>
<div id="option-1-record-metadata" class="section level2">
<h2>Option 1: Record metadata</h2>
<p>If your large data files are modified throughout the project, one option would be to record metadata about the data files, save it in a plain text file, and then commit the plain text file to the Git repository. For example, you could record the modification date, file size, <a href="https://en.wikipedia.org/wiki/MD5">MD5 checksum</a>, number of rows, number of columns, column means, etc.</p>
<p>For example, if your data file contains observational measurements from a remote sensor, you could record the date of the last observation and commit this information. Then if you need to reproduce an analysis from six months ago, you could recreate the previous version of the data file by filtering on the date column.</p>
</div>
<div id="option-2-use-git-lfs-large-file-storage" class="section level2">
<h2>Option 2: Use Git LFS (Large File Storage)</h2>
<p>If you are comfortable using Git in the terminal, a good option is <a href="https://git-lfs.com/">Git LFS</a>. It is an extension to Git that adds extra functionality to the standard Git commands. Thus it is completely compatible with workflowr.</p>
<p>Instead of committing the large file to the Git repository, it instead commits a plain text file containing a unique hash. It then uploads the large file to a remote server. If you checkout a previous version of the code, it will use the unique hash in the file to download the previous version of the large data file from the server.</p>
<p>Git LFS is <a href="https://help.github.com/en/github/managing-large-files/about-storage-and-bandwidth-usage">integrated into GitHub</a>. However, a free account is only allotted 1 GB of free storage and 1 GB a month of free bandwidth. Thus you may have to upgrade to a paid GitHub account if you need to version lots of large data files.</p>
<p>See the <a href="https://git-lfs.com/">Git LFS</a> website to download the software and set it up to track your large data files.</p>
<p>Note that for workflowr you can’t use Git LFS with any of the website files in <code>docs/</code>. <a href="https://pages.github.com/">GitHub Pages</a> serves the website using the exact versions of the files in that directory on GitHub. In other words, it won’t pull the large data files from the LFS server. Therefore everything will look fine on your local machine, but break once pushed to GitHub.</p>
<p>As an example of a workflowr project that uses Git LFS, see the GitHub repository <a href="https://github.com/jdblischak/singlecell-qtl">singlecell-qtl</a>. Note that the large data files, e.g. <a href="https://github.com/jdblischak/singlecell-qtl/blob/master/data/eset/02192018.rds"><code>data/eset/02192018.rds</code></a> , contain the phrase “Stored with Git LFS”. If you download the repository with <code>git clone</code>, the large data files will only contain the unique hashes. See the <a href="https://jdblischak.github.io/singlecell-qtl/contributing.html">contributing instructions</a> for how to use Git LFS to download the latest version of the large data files.</p>
</div>
<div id="option-3-use-piggyback" class="section level2">
<h2>Option 3: Use piggyback</h2>
<p>An alternative option to Git LFS is the R package <a href="https://cran.r-project.org/package=piggyback">piggyback</a>. Its main advantages are that it doesn’t require paying to upgrade your GitHub account or configuring Git. Instead, it uses R functions to upload large data files to <a href="https://help.github.com/en/github/administering-a-repository/about-releases">releases</a> on your GitHub repository. The main disadvantage, especially for workflowr, is that it isn’t integrated with Git. Therefore you will have to manually version the large data files by uploading them via piggyback, and recording the release version in a file in the workflowr project. This option is recommended if you anticipate substantial, but infrequent, changes to your large data files.</p>
</div>
<div id="option-4-use-a-database" class="section level2">
<h2>Option 4: Use a database</h2>
<p>Importing large amounts of data into an R session can drastically degrade R’s performance or even cause it to crash. If you have a large amount of data stored in one or more tabular files, but only need to access a subset at a time, you should consider converting your large data files into a single database. Then you can query the database from R to obtain a given subset of the data needed for a particular analysis. Not only is this memory efficient, but you will benefit from the improved organization of your project’s data. See the CRAN Task View on <a href="https://cran.r-project.org/view=Databases">Databases</a> for resources for interacting with databases with R.</p>
</div>



<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
