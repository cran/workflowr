<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />

<meta name="viewport" content="width=device-width, initial-scale=1" />

<meta name="author" content="John Blischak" />

<meta name="date" content="2025-08-18" />

<title>Using large data files with workflowr</title>

<script>// Pandoc 2.9 adds attributes on both header and div. We remove the former (to
// be compatible with the behavior of Pandoc < 2.8).
document.addEventListener('DOMContentLoaded', function(e) {
  var hs = document.querySelectorAll("div.section[class*='level'] > :first-child");
  var i, h, a;
  for (i = 0; i < hs.length; i++) {
    h = hs[i];
    if (!/^h[1-6]$/i.test(h.tagName)) continue;  // it should be a header h1-h6
    a = h.attributes;
    while (a.length > 0) h.removeAttribute(a[0].name);
  }
});
</script>

<style type="text/css">
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
span.underline{text-decoration: underline;}
div.column{display: inline-block; vertical-align: top; width: 50%;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
</style>







<style type="text/css">body {
background-color: #fff;
margin: 1em auto;
max-width: 700px;
overflow: visible;
padding-left: 2em;
padding-right: 2em;
font-family: "Open Sans", "Helvetica Neue", Helvetica, Arial, sans-serif;
font-size: 14px;
line-height: 1.35;
}
#TOC {
clear: both;
margin: 0 0 10px 10px;
padding: 4px;
width: 400px;
border: 1px solid #CCCCCC;
border-radius: 5px;
background-color: #f6f6f6;
font-size: 13px;
line-height: 1.3;
}
#TOC .toctitle {
font-weight: bold;
font-size: 15px;
margin-left: 5px;
}
#TOC ul {
padding-left: 40px;
margin-left: -1.5em;
margin-top: 5px;
margin-bottom: 5px;
}
#TOC ul ul {
margin-left: -2em;
}
#TOC li {
line-height: 16px;
}
table {
margin: 1em auto;
border-width: 1px;
border-color: #DDDDDD;
border-style: outset;
border-collapse: collapse;
}
table th {
border-width: 2px;
padding: 5px;
border-style: inset;
}
table td {
border-width: 1px;
border-style: inset;
line-height: 18px;
padding: 5px 5px;
}
table, table th, table td {
border-left-style: none;
border-right-style: none;
}
table thead, table tr.even {
background-color: #f7f7f7;
}
p {
margin: 0.5em 0;
}
blockquote {
background-color: #f6f6f6;
padding: 0.25em 0.75em;
}
hr {
border-style: solid;
border: none;
border-top: 1px solid #777;
margin: 28px 0;
}
dl {
margin-left: 0;
}
dl dd {
margin-bottom: 13px;
margin-left: 13px;
}
dl dt {
font-weight: bold;
}
ul {
margin-top: 0;
}
ul li {
list-style: circle outside;
}
ul ul {
margin-bottom: 0;
}
pre, code {
background-color: #f7f7f7;
border-radius: 3px;
color: #333;
white-space: pre-wrap; 
}
pre {
border-radius: 3px;
margin: 5px 0px 10px 0px;
padding: 10px;
}
pre:not([class]) {
background-color: #f7f7f7;
}
code {
font-family: Consolas, Monaco, 'Courier New', monospace;
font-size: 85%;
}
p > code, li > code {
padding: 2px 0px;
}
div.figure {
text-align: center;
}
img {
background-color: #FFFFFF;
padding: 2px;
border: 1px solid #DDDDDD;
border-radius: 3px;
border: 1px solid #CCCCCC;
margin: 0 5px;
}
h1 {
margin-top: 0;
font-size: 35px;
line-height: 40px;
}
h2 {
border-bottom: 4px solid #f7f7f7;
padding-top: 10px;
padding-bottom: 2px;
font-size: 145%;
}
h3 {
border-bottom: 2px solid #f7f7f7;
padding-top: 10px;
font-size: 120%;
}
h4 {
border-bottom: 1px solid #f7f7f7;
margin-left: 8px;
font-size: 105%;
}
h5, h6 {
border-bottom: 1px solid #ccc;
font-size: 105%;
}
a {
color: #0033dd;
text-decoration: none;
}
a:hover {
color: #6666ff; }
a:visited {
color: #800080; }
a:visited:hover {
color: #BB00BB; }
a[href^="http:"] {
text-decoration: underline; }
a[href^="https:"] {
text-decoration: underline; }

code > span.kw { color: #555; font-weight: bold; } 
code > span.dt { color: #902000; } 
code > span.dv { color: #40a070; } 
code > span.bn { color: #d14; } 
code > span.fl { color: #d14; } 
code > span.ch { color: #d14; } 
code > span.st { color: #d14; } 
code > span.co { color: #888888; font-style: italic; } 
code > span.ot { color: #007020; } 
code > span.al { color: #ff0000; font-weight: bold; } 
code > span.fu { color: #900; font-weight: bold; } 
code > span.er { color: #a61717; background-color: #e3d2d2; } 
</style>




</head>

<body>




<h1 class="title toc-ignore">Using large data files with workflowr</h1>
<h3 class="subtitle">workflowr version 1.7.2</h3>
<h4 class="author">John Blischak</h4>
<h4 class="date">2025-08-18</h4>


<div id="TOC">
<ul>
<li><a href="#introduction" id="toc-introduction">Introduction</a></li>
<li><a href="#option-0-reconsider-versioning-your-large-data-files" id="toc-option-0-reconsider-versioning-your-large-data-files">Option 0:
Reconsider versioning your large data files</a></li>
<li><a href="#option-1-record-metadata" id="toc-option-1-record-metadata">Option 1: Record metadata</a></li>
<li><a href="#option-2-use-git-lfs-large-file-storage" id="toc-option-2-use-git-lfs-large-file-storage">Option 2: Use Git LFS
(Large File Storage)</a></li>
<li><a href="#option-3-use-piggyback" id="toc-option-3-use-piggyback">Option 3: Use piggyback</a></li>
<li><a href="#option-4-use-a-database" id="toc-option-4-use-a-database">Option 4: Use a database</a></li>
</ul>
</div>

<div id="introduction" class="section level2">
<h2>Introduction</h2>
<p>Workflowr provides many features to track the progress of your data
analysis project and make it easier to reproduce both the current
version as well as previous versions of the project. However, this is
only possible if the data files from previous versions can also be
restored. In other words, even if you can obtain the code from six
months ago, if you can’t obtain the data from six months ago, you won’t
be able to reproduce your previous analysis.</p>
<p>Unfortunately, if you have large data files, you can’t simply commit
them to the Git repository along with the code. The max file size able
to be pushed to GitHub is <a href="https://help.github.com/en/github/managing-large-files/conditions-for-large-files">100
MB</a>, and this is in general a good practice to follow no matter what
Git hosting service you are using. Large files will make each push and
pull take much longer and increase the risk of the download timing out.
This vignette discusses various strategies for versioning your large
data files.</p>
</div>
<div id="option-0-reconsider-versioning-your-large-data-files" class="section level2">
<h2>Option 0: Reconsider versioning your large data files</h2>
<p>Before considering any of the options below, you need to reconsider
if this is even necessary for your project. And if it is, which data
files need to be versioned. Specifically, large raw data files that are
never modified do not need to be versioned. Instead, you could follow
these steps:</p>
<ol style="list-style-type: decimal">
<li>Upload the files to an online data repository, a private FTP server,
etc.</li>
<li>Add a script to your workflowr project that can download all the
files</li>
<li>Include the instructions in your README and your workflowr website
that explain how to download the files</li>
</ol>
<p>For example, an <a href="https://en.wikipedia.org/wiki/RNA-Seq">RNA
sequencing</a> project will produce <a href="https://en.wikipedia.org/wiki/FASTQ_format">FASTQ</a> files that
are large and won’t be modified. Instead of committing these files to
the Git repository, they should instead be uploaded to <a href="https://www.ncbi.nlm.nih.gov/geo/">GEO</a>/<a href="https://www.ncbi.nlm.nih.gov/sra">SRA</a>.</p>
</div>
<div id="option-1-record-metadata" class="section level2">
<h2>Option 1: Record metadata</h2>
<p>If your large data files are modified throughout the project, one
option would be to record metadata about the data files, save it in a
plain text file, and then commit the plain text file to the Git
repository. For example, you could record the modification date, file
size, <a href="https://en.wikipedia.org/wiki/MD5">MD5 checksum</a>,
number of rows, number of columns, column means, etc.</p>
<p>For example, if your data file contains observational measurements
from a remote sensor, you could record the date of the last observation
and commit this information. Then if you need to reproduce an analysis
from six months ago, you could recreate the previous version of the data
file by filtering on the date column.</p>
</div>
<div id="option-2-use-git-lfs-large-file-storage" class="section level2">
<h2>Option 2: Use Git LFS (Large File Storage)</h2>
<p>If you are comfortable using Git in the terminal, a good option is <a href="https://git-lfs.com/">Git LFS</a>. It is an extension to Git that
adds extra functionality to the standard Git commands. Thus it is
completely compatible with workflowr.</p>
<p>Instead of committing the large file to the Git repository, it
instead commits a plain text file containing a unique hash. It then
uploads the large file to a remote server. If you checkout a previous
version of the code, it will use the unique hash in the file to download
the previous version of the large data file from the server.</p>
<p>Git LFS is <a href="https://help.github.com/en/github/managing-large-files/about-storage-and-bandwidth-usage">integrated
into GitHub</a>. However, a free account is only allotted 1 GB of free
storage and 1 GB a month of free bandwidth. Thus you may have to upgrade
to a paid GitHub account if you need to version lots of large data
files.</p>
<p>See the <a href="https://git-lfs.com/">Git LFS</a> website to
download the software and set it up to track your large data files.</p>
<p>Note that for workflowr you can’t use Git LFS with any of the website
files in <code>docs/</code>. <a href="https://pages.github.com/">GitHub
Pages</a> serves the website using the exact versions of the files in
that directory on GitHub. In other words, it won’t pull the large data
files from the LFS server. Therefore everything will look fine on your
local machine, but break once pushed to GitHub.</p>
<p>As an example of a workflowr project that uses Git LFS, see the
GitHub repository <a href="https://github.com/jdblischak/singlecell-qtl">singlecell-qtl</a>.
Note that the large data files, e.g. <a href="https://github.com/jdblischak/singlecell-qtl/blob/master/data/eset/02192018.rds"><code>data/eset/02192018.rds</code></a>
, contain the phrase “Stored with Git LFS”. If you download the
repository with <code>git clone</code>, the large data files will only
contain the unique hashes. See the <a href="https://jdblischak.github.io/singlecell-qtl/contributing.html">contributing
instructions</a> for how to use Git LFS to download the latest version
of the large data files.</p>
</div>
<div id="option-3-use-piggyback" class="section level2">
<h2>Option 3: Use piggyback</h2>
<p>An alternative option to Git LFS is the R package <a href="https://cran.r-project.org/package=piggyback">piggyback</a>. Its
main advantages are that it doesn’t require paying to upgrade your
GitHub account or configuring Git. Instead, it uses R functions to
upload large data files to <a href="https://help.github.com/en/github/administering-a-repository/about-releases">releases</a>
on your GitHub repository. The main disadvantage, especially for
workflowr, is that it isn’t integrated with Git. Therefore you will have
to manually version the large data files by uploading them via
piggyback, and recording the release version in a file in the workflowr
project. This option is recommended if you anticipate substantial, but
infrequent, changes to your large data files.</p>
</div>
<div id="option-4-use-a-database" class="section level2">
<h2>Option 4: Use a database</h2>
<p>Importing large amounts of data into an R session can drastically
degrade R’s performance or even cause it to crash. If you have a large
amount of data stored in one or more tabular files, but only need to
access a subset at a time, you should consider converting your large
data files into a single database. Then you can query the database from
R to obtain a given subset of the data needed for a particular analysis.
Not only is this memory efficient, but you will benefit from the
improved organization of your project’s data. See the CRAN Task View on
<a href="https://cran.r-project.org/view=Databases">Databases</a> for
resources for interacting with databases with R.</p>
</div>



<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
